{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path, tokenizer = None):\n",
    "    whole_texts = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n",
    "        \n",
    "    if tokenizer == None :\n",
    "        tokenizer = Tokenizer(filters=\"\")\n",
    "        tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n",
    "\n",
    "x_train = np.load('data/x_train.npy')\n",
    "x_valid = np.load('data/x_valid.npy')\n",
    "\n",
    "# 読み込み＆Tokenizerによる数値化\n",
    "y_train, tokenizer_train = load_data('data/y_train.txt')\n",
    "y_valid, _ = load_data('data/y_valid.txt', tokenizer_train)\n",
    "\n",
    "vocab_size = len(tokenizer_train.word_index) + 1\n",
    "\n",
    "# パディング\n",
    "y_train = pad_sequences(y_train, padding='post')\n",
    "y_valid = pad_sequences(y_valid, padding='post')\n",
    "\n",
    "caption_len = len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Flatten, Lambda\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "encoder_input = Input(shape=(224, 224, 3))\n",
    "encoder_input_normalized = Lambda(lambda x: x / 255.)(encoder_input) # [0, 255) -> [0, 1)\n",
    "encoder = VGG16(weights='imagenet', include_top=False, input_tensor=encoder_input_normalized)\n",
    "\n",
    "# パラメータを固定\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# CNNの出力\n",
    "u = Flatten()(encodero.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hid_dim = 128\n",
    "\n",
    "# LSTMの初期状態\n",
    "decoded_states = [Dense(hid_dim)(u), Dense(hid_dim)(u)] # h_0、c_0 に対応\n",
    "\n",
    "# 層の定義\n",
    "decoder_input = Input(shape=(caption_len,))\n",
    "embedding = Embedding(vocab_size, emb_dim, mask_zero=True)\n",
    "lstm = LSTM(hid_dim, activation='tanh', return_sequences=True, return_state=True)\n",
    "\n",
    "# 層の接続\n",
    "decoder_embedded = embedding(decoder_input)\n",
    "decoder_output, _, _ = lstm(decoder_embedded, initial_state=decoded_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Activation, concatenate, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. reshape: (7, 7, 512) -> (49, 512)\n",
    "u_map = Reshape((7*7, 512))(u)\n",
    "\n",
    "# 1. スコアの計算\n",
    "dense_att = Dense(hid_dim)\n",
    "score = dot([decoder_output, dense_att(u_map)], axes=-1)\n",
    "\n",
    "# 2. 重みの計算\n",
    "attention = Activation('softmax')(score)\n",
    "\n",
    "# 3. 文脈ベクトルの計算\n",
    "context = dot([attention_umap], axes=(2, 1))\n",
    "\n",
    "# 4. 出力ベクトルの計算\n",
    "attention_dense = Dense(hid_dim, activation='tanh')\n",
    "output_dense = Dense(vocab_size, activation='softmax')\n",
    "concat = concatenate([context, decoder_output], axis=2)\n",
    "attentional = attention_dense(concat)\n",
    "y_pred = output_dense(attentional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model([encoder_input, decoder_input], y_pred)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
    "\n",
    "model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=64, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, [u_map]+decoded_states)\n",
    "\n",
    "decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
    "\n",
    "decoder_input = Input(shape=(1,))\n",
    "decoder_embeded = embedding(decoder_input)\n",
    "decoded_seq, *decoder_states = lstm(decoder_embeded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoded_seq] + decoder_states)\n",
    "\n",
    "# Attention\n",
    "u_map_in, decoded_seq_in = Input(shape=(7*7, 512)), Input(shape=(1, hid_dim))\n",
    "score = dot([decoded_seq_in, dense_att(u_map_in)], axes=-1)\n",
    "attention = Activation('softmax')(score)\n",
    "context = dot([attention, u_map_in], axes=(2,1))\n",
    "concat = concatenate([context, decoded_seq_in], axis=2)\n",
    "attentional = attention_dense(concat)\n",
    "attention_outputs = output_dense(attentional)\n",
    "\n",
    "attention_model = Model([u_map_in, decoded_seq_in], [attention_outputs, attention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logの中身が0になるのを防ぐ\n",
    "def np_log(x):\n",
    "    return np.log(np.clip(x, 1e-10, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from utils import Node\n",
    "\n",
    "def beam_search(input_image, bos_eos, K=5, max_output_length = 100):\n",
    "    u_map, *states = encoder_model.predict(input_image)\n",
    "    \n",
    "    # [score, y_pred, attention, states_tm1]\n",
    "    candidates = [[0, np.array(bos_eos[0]), np.empty((0, 7*7)), states]]\n",
    "    root = Node(bos_eos[0][0])\n",
    "        \n",
    "    t = 0\n",
    "    while t < max_output_length:\n",
    "        t += 1\n",
    "        \n",
    "        # すべての候補を一時的に保管するリスト\n",
    "        tmp_candidates = []\n",
    "        \n",
    "        # </s>がすべての候補で出力されたかどうかのフラッグ\n",
    "        end_flag = True\n",
    "        for score_sum, y_pred, attention_seq, states_prev in candidates:\n",
    "            attention_seq = copy.deepcopy(attention_seq)\n",
    "            if y_pred[-1] == bos_eos[1]:\n",
    "                tmp_candidates.append([score_sum, y_pred, attention_seq, states_prev])\n",
    "            else:\n",
    "                end_flag = False\n",
    "                decoded_seq, *states = decoder_model.predict([y_pred[-1:]] + states_prev)\n",
    "                token_dist, attention = attention_model.predict([u_map, decoded_seq])\n",
    "                \n",
    "                # 確率の高い単語（＝対数尤度の高い単語）とそのidを取得（上位K個）\n",
    "                y_loglikelihood = np_log(token_dist.flatten())\n",
    "                y_t, s_t = np.argsort(y_loglikelihood)[::-1][:K], np.sort(y_loglikelihood)[::-1][:K] # argsortは昇順なので反転\n",
    "                \n",
    "                # スコア (対数尤度) を蓄積（s_core_tm1はスカラー、s_tはベクトル）\n",
    "                s_t = s_t + score_sum\n",
    "                \n",
    "                # すべての候補を一時的に保管\n",
    "                tmp_candidates.extend(\n",
    "                    [[s_tk, np.append(y_pred, [y_tk]), np.append(attention_seq, [attention.flatten()], axis=0), states] for s_tk, y_tk in zip(s_t, y_t)]\n",
    "                )\n",
    "        if end_flag:\n",
    "            break\n",
    "        \n",
    "        # 正規化したスコアでソートし、上位K個の候補を保存 ()\n",
    "        candidates = sorted(tmp_candidates, key=lambda x: x[0]/len(x[1]), reverse=True)[:K]\n",
    "        \n",
    "        # Beam Search可視化用\n",
    "        root.depth += 1\n",
    "        for _, y_pred, _, _ in candidates:\n",
    "            root.add_child(y_pred)\n",
    "    \n",
    "    # 最もスコアの高い候補を返す\n",
    "    return candidates[0][1], root, candidates[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_image, bos_eos, max_output_length = 100):\n",
    "    u_map, *states = encoder_model.predict(input_image)\n",
    "\n",
    "    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"に対応するインデックス\n",
    "    output_seq = bos_eos[0]\n",
    "    attention_seq = np.empty((0, 7*7))\n",
    "    \n",
    "    while True:\n",
    "        decoded_seq, *states = decoder_model.predict([target_seq] + states)\n",
    "        token_dist, attention = attention_model.predict([u_map, decoded_seq])\n",
    "        sampled_token_index = [np.argmax(token_dist[0, -1, :])]\n",
    "        output_seq += sampled_token_index\n",
    "        attention_seq = np.append(attention_seq, [attention.flatten()], axis=0)\n",
    "        \n",
    "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array(sampled_token_index)\n",
    "\n",
    "    return output_seq, attention_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_no = 100\n",
    "\n",
    "bos_eos = tokenizer_train.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "detokenizer_train = dict(map(reversed, tokenizer_train.word_index.items()))\n",
    "\n",
    "# x_test = np.array(x_valid[test_no:test_no+1])\n",
    "x_test = np.array(x_train[test_no:test_no+1])\n",
    "\n",
    "y_pred, root, att_a= beam_search(x_test, bos_eos, K=3, max_output_length=25)\n",
    "#y_pred, att_a= decode_sequence(x_test, bos_eos, max_output_length=25)\n",
    "\n",
    "detokenizer_train[0] = \"\"\n",
    "print(' '.join([detokenizer_train[i] for i in y_pred]))\n",
    "\n",
    "plt.imshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "for i, (wordid, att_a_t) in enumerate(zip(y_pred[1:], att_a)):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    \n",
    "    # Plot image\n",
    "    ax.imshow(x_test[0])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Plot attention\n",
    "    att_a_t = skimage.transform.pyramid_expand(att_a_t.reshape(7, 7), upscale=32, sigma=20, multichannel=False)\n",
    "    ax.imshow(att_a_t, alpha=.65)\n",
    "    # Plot word\n",
    "    ax.set_title(detokenizer_train[wordid])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = fig.add_subplot(111, xticks=[], yticks=[])\n",
    "ax.axis('off')\n",
    "\n",
    "root.mark_best_path(y_pred)\n",
    "\n",
    "root.set_coordinates(0, 0, ax, detokenizer_train, tree_depth=root.depth)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
